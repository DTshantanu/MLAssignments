{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############\n",
    "## DATA IO ##\n",
    "#############\n",
    "\n",
    "def get_data(filepath):\n",
    "    # Opens the file handler for the dataset file. Using variable 'f' we can access and manipulate our file anywhere \n",
    "    # in our code after the next code line.\n",
    "    f = open(filepath,\"r\")\n",
    "\n",
    "    # Predictors Collection (or your input variable) (which in this case is the Sepal length, Sepal width, Petal length and Petal width)\n",
    "    X = [[],[],[],[]]\n",
    "\n",
    "    # Output Response (or your output variable) (which in this case is the category to which the iris flower belongs.\n",
    "    # Remember: We are interested in seperating Setosa category [label - 1] from other non-setosa categories [label - 0].)\n",
    "    Y = []\n",
    "\n",
    "    # Initializing a reader generator using reader method from csv module. A reader generator takes each line from the \n",
    "    # file and converts it into list of columns.\n",
    "    reader = csv.reader(f)\n",
    "\n",
    "    # Using for loop, we are able to read one row at a time.\n",
    "    for row in reader:\n",
    "        # For extracting out 4 input variables i.e Sepal length, Sepal width, Petal length, Petal width\n",
    "        for i in range(0,4):\n",
    "            X[i].append(float(row[i]))\n",
    "        \n",
    "        # For extracting output category (Y) i.e 1 for Setosa and 0 for Non-Setosa\n",
    "        if row[4] == \"Iris-setosa\":\n",
    "            Y.append(1)\n",
    "        else:\n",
    "            Y.append(0)\n",
    "\n",
    "    # Close the file once we have succesffuly stored all data into our X and Y variables.\n",
    "    f.close()\n",
    "    \n",
    "    # Normalization of Input Data\n",
    "    mean=[]\n",
    "    \n",
    "    std=[]\n",
    "    \n",
    "    for i in range(0,4):\n",
    "        mean.append(np.mean(X[i]))\n",
    "        std.append(np.std(X[i]))\n",
    "\n",
    "    return [np.array([(X[i] - mean[i])/(std[i]) for i in range(0,4)]),np.array(Y),mean,std]\n",
    "\n",
    "\n",
    "# Function for normalized output\n",
    "def normalize_for_output(x,mean,std):\n",
    "    return [(x[i] - mean[i])/(std[i]) for i in range(0,4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic(a):\n",
    "    return (1/(1+np.exp(-1*a)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################\n",
    "## Error Calculation ##\n",
    "#####################\n",
    "\n",
    "def error(x, y, betas):\n",
    "    error = 0\n",
    "    for i in range(x[0].shape[0]):\n",
    "        predicted_value = logistic(betas[0] + (betas[1] * x[0][i])+(betas[2] * x[1][i])+(betas[3] * x[2][i])+(betas[4] * x[3][i]))\n",
    "        actual_value = y[i]\n",
    "        error = error + ((-predicted_value*np.log(predicted_value))-((1-actual_value)*np.log(1-predicted_value)))\n",
    "    return (error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predicted_value_final(X,i,betas):\n",
    "    return (logistic(betas[0] + (betas[1] * X[0][i]) + (betas[2] * X[1][i]) + (betas[3] * X[2][i]) + (betas[4] * X[3][i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientDescentAlgorithm(x, y, learning_rate):\n",
    "    \n",
    "    print (\"Training Linear Regression Model using Gradient Descent\")\n",
    "    \n",
    "    maximum_iterations = 100\n",
    "    \n",
    "    # This flag lets the program know wether the gradient descent algorithm has reached it's converged state which means wether \n",
    "    # the algorithm was able to find the local minima (where the slope of RSS wrt your parameters beta_0 and beta_1 is zero)\n",
    "    converge_status = False\n",
    "    \n",
    "    # num_rows stores the number of datapoints in the current dataset provided for training.\n",
    "    num_rows = x[0].shape[0]\n",
    "\n",
    "    # Initial Value of parameters \n",
    "    betas = [0,0,0,0,0]\n",
    "    \n",
    "    # Initial Error or RSS(beta_0,beta_1) based on the initial parameter values\n",
    "    #error = RSS(x, y, beta_0, beta_1)\n",
    "    _error = error(x, y, betas)\n",
    "    #print('Initial Value (Cost Function)=', error);\n",
    "    \n",
    "    # Iterate Loop\n",
    "    num_iter = 0\n",
    "    while not converge_status:\n",
    "        # for each training sample, compute the gradient (d/d_beta j(beta))\n",
    "        gradient_0 = 1.0/num_rows * sum([(predicted_value_final(x,i,betas) - y[i]) for i in range(num_rows)]) \n",
    "        gradient_1 = 1.0/num_rows * sum([(predicted_value_final(x,i,betas) - y[i])*x[0][i] for i in range(num_rows)])\n",
    "        gradient_2 = 1.0/num_rows * sum([(predicted_value_final(x,i,betas) - y[i])*x[1][i] for i in range(num_rows)]) \n",
    "        gradient_3 = 1.0/num_rows * sum([(predicted_value_final(x,i,betas) - y[i])*x[2][i] for i in range(num_rows)])\n",
    "        gradient_4 = 1.0/num_rows * sum([(predicted_value_final(x,i,betas) - y[i])*x[3][i] for i in range(num_rows)]) \n",
    "        \n",
    "        # Computation of new parameters according to the current gradient.\n",
    "        temp0 = betas[0] - learning_rate * gradient_0\n",
    "        temp1 = betas[1] - learning_rate * gradient_1\n",
    "        temp2 = betas[2] - learning_rate * gradient_2\n",
    "        temp3 = betas[3] - learning_rate * gradient_3\n",
    "        temp4 = betas[4] - learning_rate * gradient_4\n",
    "\n",
    "    \n",
    "        # Simultaneous Update of Parameters Beta_0 and Beta_1.\n",
    "        betas[0] = temp0\n",
    "        betas[1] = temp1\n",
    "        betas[2] = temp2\n",
    "        betas[3] = temp3\n",
    "        betas[4] = temp4\n",
    "\n",
    "\n",
    "        \n",
    "        current_error = error(x, y, betas)\n",
    "        \n",
    "        if num_iter % 10 == 0:\n",
    "            print ('Current Value of RSS (Cost Function) based on updated values= ',  current_error)\n",
    "            \n",
    "        _error = current_error   # update error \n",
    "        num_iter = num_iter + 1  # update iter\n",
    "    \n",
    "        if num_iter == maximum_iterations:\n",
    "            print (\"Training Interrupted as Maximum number of iterations were crossed.\\n\\n\")\n",
    "            converge_status = True\n",
    "    \n",
    "    return (betas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_probability(coef,X):\n",
    "\tbeta_0 = coef[0]\n",
    "\tbeta_1 = coef[1]\n",
    "\t# Change 11\n",
    "\tbeta_2 = coef[2]\n",
    "\tbeta_3 = coef[3]\n",
    "\tbeta_4 = coef[4]\n",
    "    \n",
    "\tfy = []\n",
    "\tif len(X) > 1:\n",
    "\t\tfor x in X:\n",
    "\t\t\ta = beta_0 + (beta_1 * x[0]) + (beta_2 * x[1]) + (beta_3 * x[2]) + (beta_4 * x[3])\n",
    "\t\t\tresult = logistic(a)\n",
    "\t\t\tprint(x[0],x[1],x[2],x[3],a,result)\n",
    "\t\t\tfy.append(result)\n",
    "\t\treturn fy\n",
    "\n",
    "\t# Our Regression Model defined using the coefficients from slr function\n",
    "\tx = X[0]\n",
    "\tY = logistic(beta_0 + (beta_1 * x[0]) + (beta_2 * x[1]) + (beta_3 * x[2]) + (beta_4 * x[3]))\n",
    "\n",
    "\treturn Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(predicted_probability,threshold = 0.5):\n",
    "    if predicted_probability > threshold:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_confusion_matrix(test_Y,pred_Y):\n",
    "    tp = (pred_Y[test_Y == 1])\n",
    "    tp = tp[tp==1]\n",
    "    tp = tp.shape[0]\n",
    "    \n",
    "    tn = (pred_Y[test_Y == 0])\n",
    "    tn = tn[tn==0]\n",
    "    tn = tn.shape[0]\n",
    "    \n",
    "    fn = 90 - tn\n",
    "    fp = 45 - tp\n",
    "    \n",
    "    return [tp,fp,tn,fn]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on components of confusion matrix, we compute accuracy, precision and recall evaluation metrics.\n",
    "def evaluate_performance(test_Y,pred_Y):\n",
    "    tp,fp,tn,fn = get_confusion_matrix(test_Y,pred_Y)\n",
    "    \n",
    "    accuracy = (tp + tn) / (tp + fp + tn + fn)\n",
    "    \n",
    "    # Precision of Positive class = (TP / TP + FP) \n",
    "    # Similarly precision of negative class = (TN / TN + FN)\n",
    "    # Precision = Avg (Precision of Positive Class, Precision of Negative Class)\n",
    "    precision = ((tp / (tp + fp)) + (tn / (tn + fn)))/2\n",
    "    \n",
    "    # Recall of Positive class = (TP / TP + FN) \n",
    "    # Similarly Recall of Negative class = (TN / TN + FP)\n",
    "    # Recall = Avg (Recall of Positive Class, Recall of Negative Class)\n",
    "    recall = ((tp/(tp+fn)) + (tn/(tn+fp)))/2\n",
    "    \n",
    "    return [round(accuracy*100,2),round(precision*100,2),round(recall*100,2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Linear Regression Model using Gradient Descent\n",
      "('Current Value of RSS (Cost Function) based on updated values= ', 11.542839946499335)\n",
      "('Current Value of RSS (Cost Function) based on updated values= ', 7.7123975312048945)\n",
      "('Current Value of RSS (Cost Function) based on updated values= ', 5.8736486902746625)\n",
      "('Current Value of RSS (Cost Function) based on updated values= ', 4.812591291653202)\n",
      "('Current Value of RSS (Cost Function) based on updated values= ', 4.115335565632171)\n",
      "('Current Value of RSS (Cost Function) based on updated values= ', 3.6173777989538944)\n",
      "('Current Value of RSS (Cost Function) based on updated values= ', 3.241175113990322)\n",
      "('Current Value of RSS (Cost Function) based on updated values= ', 2.9452963658621387)\n",
      "('Current Value of RSS (Cost Function) based on updated values= ', 2.7054867172557127)\n",
      "('Current Value of RSS (Cost Function) based on updated values= ', 2.5065370845406134)\n",
      "Training Interrupted as Maximum number of iterations were crossed.\n",
      "\n",
      "\n",
      "('Final Values for Beta Parameters are (from beta_0 to beta_4) :', [-0.6539124875804105, -0.9550378946465553, 0.5573476027764889, -1.0621091630209138, -1.0163336774276517])\n"
     ]
    }
   ],
   "source": [
    "X,Y,mean,std = get_data(\"iris.csv\")\n",
    "\n",
    "train_X = np.concatenate((X[:,0:5],X[:,50:55],X[:,100:105]),1)\n",
    "#print(train_X)\n",
    "test_X = np.concatenate((X[:,5:50],X[:,55:100],X[:,105:150]),1)\n",
    "##print(test_X)\n",
    "\n",
    "train_Y = np.concatenate((Y[0:5],Y[50:55],Y[100:105]),0)\n",
    "test_Y = np.concatenate((Y[5:50],Y[55:100],Y[105:150]),0)\n",
    "\n",
    "\n",
    "################################################\n",
    "## Model Training (or coefficient estimation) ##\n",
    "################################################\n",
    "# Using our gradient descent function we estimate coefficients of our regression line. The gradient descent function \n",
    "# returns a list of coefficients\n",
    "\n",
    "coefficients = gradientDescentAlgorithm(train_X,train_Y,0.1)\n",
    "\n",
    "########################\n",
    "## Making Predictions ##\n",
    "########################\n",
    "\n",
    "# Using our predict function and the coefficients given by our gradient descent function we can now predict the time it will take\n",
    "# for the next eruption.\n",
    "print (\"Final Values for Beta Parameters are (from beta_0 to beta_4) :\",coefficients)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Accuracy:', 0.0, 'Precision:', 0.0, 'Recall:', 0.0)\n"
     ]
    }
   ],
   "source": [
    "pred_Y = []\n",
    "for i in range(0,np.transpose(test_X).shape[0]):\n",
    "    probability = predict_probability(coefficients,[np.transpose(test_X)[i]])\n",
    "    pred_Y.append(classify(probability))\n",
    "    \n",
    "pred_Y = np.array(pred_Y)\n",
    "    \n",
    "accuracy,precision,recall = evaluate_performance(test_Y,pred_Y)\n",
    "\n",
    "print (\"Accuracy:\",accuracy,\"Precision:\",precision,\"Recall:\",recall)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
